{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-16 20:01:46\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "print(start.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Libraries\n",
    "import findspark\n",
    "findspark.init('/home/cse587/spark-2.4.0-bin-hadoop2.7')\n",
    "\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SQLContext\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, CountVectorizer, IDF, Word2Vec, HashingTF\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, lower, regexp_replace, when, lit, concat_ws\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =  pyspark.SparkContext()\n",
    "\n",
    "memory = '10g'\n",
    "pyspark_submit_args = ' --driver-memory ' + memory + ' pyspark-shell'\n",
    "os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args\n",
    "\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Pyspark demo\") \\\n",
    "    .config(\"spark.some.config.option\", \"some-value\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data\n",
    "d1 = pd.read_csv('/home/cse587/dic487-587/train.csv')\n",
    "train = sqlContext.createDataFrame(d1)\n",
    "\n",
    "d2 = pd.read_csv('/home/cse587/dic487-587/test.csv')\n",
    "test = sqlContext.createDataFrame(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-16 20:02:25\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print(now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocessing\n",
    "clean_plot = train.select('movie_id', 'movie_name', (lower(regexp_replace('plot', \"[^a-zA-Z\\\\s]\", \"\")).alias('plot')), 'genre')\n",
    "clean_plot_test = test.select('movie_id', 'movie_name', (lower(regexp_replace('plot', \"[^a-zA-Z\\\\s]\", \"\")).alias('plot')))\n",
    "\n",
    "tokenizer = Tokenizer(inputCol = 'plot', outputCol = 'plot_words')\n",
    "words_plot = tokenizer.transform(clean_plot).select('movie_id', 'movie_name', 'plot_words', 'genre')\n",
    "words_plot_test = tokenizer.transform(clean_plot_test).select('movie_id', 'movie_name', 'plot_words')\n",
    "\n",
    "stop_words_remover = StopWordsRemover(inputCol = 'plot_words', outputCol = 'plot_clean')\n",
    "clean_train = stop_words_remover.transform(words_plot).select('movie_id', 'movie_name', 'plot_clean', 'genre')\n",
    "clean_test = stop_words_remover.transform(words_plot_test).select('movie_id', 'movie_name', 'plot_clean')\n",
    "\n",
    "filter_len = udf(lambda row: [x for x in row if len(x) > 2], ArrayType(StringType()))\n",
    "final_train = clean_train.withColumn('plots', filter_len(col('plot_clean'))).select('movie_id', 'movie_name', 'plots', 'genre')\n",
    "final_test = clean_test.withColumn('plots', filter_len(col('plot_clean'))).select('movie_id', 'movie_name', 'plots')\n",
    "\n",
    "cv = CountVectorizer(inputCol = \"plots\", outputCol = \"feature_vector\", vocabSize = 20000, minDF = 2)\n",
    "model = cv.fit(final_train)\n",
    "train_data = model.transform(final_train)\n",
    "model = cv.fit(final_test)\n",
    "test_data = model.transform(final_test)\n",
    "\n",
    "train_data = train_data.select('feature_vector', 'genre')\n",
    "test_data = test_data.select('feature_vector')\n",
    "\n",
    "movie_id = test.select(\"movie_id\").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-16 20:03:16\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print(now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(train, test, key):\n",
    "    nnr = train.withColumn('label', when(train.genre.contains(key),1).otherwise(0))\n",
    "    data = nnr.select('features','label')\n",
    "    lr = LogisticRegression(maxIter = 100)\n",
    "    lrModel = lr.fit(data)\n",
    "    res = lrModel.transform(test)\n",
    "    pred = res.select(\"prediction\")\n",
    "    pred = pred.withColumnRenamed(\"prediction\",key).toPandas()\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part1(data, test):    \n",
    "    c00 = get_predictions(data, test, \"Drama\")\n",
    "    c01 = get_predictions(data, test, \"Comedy\")\n",
    "    c02 = get_predictions(data, test, \"Romance Film\")\n",
    "    c03 = get_predictions(data, test, \"Thriller\")\n",
    "    c04 = get_predictions(data, test, \"Action\")\n",
    "    c05 = get_predictions(data, test, \"World cinema\")\n",
    "    c06 = get_predictions(data, test, \"Crime Fiction\")\n",
    "    c07 = get_predictions(data, test, \"Horror\")\n",
    "    c08 = get_predictions(data, test, \"Black-and-white\")\n",
    "    c09 = get_predictions(data, test, \"Indie\")\n",
    "    c10 = get_predictions(data, test, \"Action/Adventure\")\n",
    "    c11 = get_predictions(data, test, \"Adventure\")\n",
    "    c12 = get_predictions(data, test, \"Family Film\")\n",
    "    c13 = get_predictions(data, test, \"Short Film\")\n",
    "    c14 = get_predictions(data, test, \"Romantic Drama\")\n",
    "    c15 = get_predictions(data, test, \"Animation\")\n",
    "    c16 = get_predictions(data, test, \"Musical\")\n",
    "    c17 = get_predictions(data, test, \"Science Fiction\")\n",
    "    c18 = get_predictions(data, test, \"Mystery\")\n",
    "    c19 = get_predictions(data, test, \"Romantic comedy\")\n",
    "    \n",
    "    final_res = pd.concat([c00, c01, c02, c03, c04, c05, c06, c07, c08, c09, c10, c11, c12, c13, c14, c15, c16, c17, c18, c19], axis = 1)\n",
    "    final_res.columns = final_res.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('-', '_').str.replace('/', '_')\n",
    "    final_res = final_res.astype(int)\n",
    "    \n",
    "    p = sqlContext.createDataFrame(final_res)\n",
    "    predictions = p.select(concat_ws(' ',p.drama, p.comedy, p.romance_film, p.thriller, p.action, p.world_cinema, p.crime_fiction, p.horror, p.black_and_white, p.indie, p.action_adventure, p.family_film, p.short_film, p.romantic_drama, p.animation, p.musical, p.science_fiction, p.mystery, p.romantic_comedy).alias(\"prediction\"))\n",
    "    predictions = predictions.withColumnRenamed('prediction','predictions')\n",
    "    predictions = predictions.toPandas()\n",
    "    predictions = pd.concat([movie_id, predictions], axis = 1)\n",
    "    return predictions\n",
    "\n",
    "train1 = train_data.select('feature_vector', 'genre')\n",
    "train1 = train1.withColumnRenamed('feature_vector','features')\n",
    "test1 = test_data.select('feature_vector')\n",
    "test1 = test1.withColumnRenamed('feature_vector','features')\n",
    "final_pred1 = part1(train1, test1)\n",
    "labels1 = sqlContext.createDataFrame(final_pred1)\n",
    "labels1.write.csv('Part1', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-16 20:11:28\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print(now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part - 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf_func(df):\n",
    "    idf = IDF(inputCol=\"feature_vector\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(df)\n",
    "    rescaledData = idfModel.transform(df)\n",
    "    return rescaledData\n",
    "\n",
    "def part2(data, test):\n",
    "    rescaled_train = idf_func(data)\n",
    "    rescaled_test = idf_func(test)\n",
    "    prediction = part1(rescaled_train, rescaled_test)\n",
    "    return prediction\n",
    "\n",
    "final_pred2 = part2(train_data, test_data)\n",
    "labels2 = sqlContext.createDataFrame(final_pred2)\n",
    "labels2.write.csv('Part2', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-16 20:18:38\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "print(now.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def part3(data, test):\n",
    "    hashingTF = HashingTF(inputCol=\"plots\", outputCol=\"feature_vector\", numFeatures=10000)\n",
    "    featurizedData = hashingTF.transform(data)\n",
    "    featurizedTest = hashingTF.transform(test)\n",
    "    rescaled_train = idf_func(featurizedData)\n",
    "    rescaled_test = idf_func(featurizedTest)\n",
    "    prediction = part1(rescaled_train, rescaled_test)\n",
    "    return prediction\n",
    "\n",
    "train3 = final_train.select('plots', 'genre')\n",
    "test3 = final_test.select('plots')\n",
    "final_pred3 = part3(train3, test3)\n",
    "labels3 = sqlContext.createDataFrame(final_pred3)\n",
    "labels3.write.csv('Part3', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-05-16 20:26:52\n"
     ]
    }
   ],
   "source": [
    "end = datetime.datetime.now()\n",
    "print(end.strftime(\"%Y-%m-%d %H:%M:%S\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
